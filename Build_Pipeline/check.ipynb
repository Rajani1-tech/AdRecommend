{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             visitorEmail                     ad_id  clickedOrNot\n",
      "0      promod@yopmail.com  64d4e9ff3709c058de9809ec             1\n",
      "1  kalipopup1@yopmail.com  64d4e66e3709c058de9809cb             0\n",
      "2  kalipopup1@yopmail.com  64d9af723709c058de980f8d             0\n",
      "3  kalipopup1@yopmail.com  64d9af9d3709c058de980f94             0\n",
      "4  kalipopup1@yopmail.com  64d9af4d3709c058de980f86             0\n",
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataProcessing:\n",
    "    \"\"\"\n",
    "    This class performs basic preprocessing steps, like columns arrangement and renaming as required in the algorithm.\n",
    "\n",
    "    Attributes:\n",
    "        csv_path (str): Path of the CSV file.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path):\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Arranging the received csv file into the format best suited for a specific algorithm.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed dataframe.\n",
    "        \"\"\"\n",
    "        # Read the CSV file into a DataFrame\n",
    "        data = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Reorder columns and rename them\n",
    "        data = data[['visitorEmail', 'ad_id', 'clickedOrNot']]\n",
    "        \n",
    "        # Replace 'Clicked' and 'Not Clicked' with 1 and 0, respectively\n",
    "        data['clickedOrNot'] = data['clickedOrNot'].replace({'Clicked': 1, 'Not Clicked': 0})\n",
    "        \n",
    "        # Handle non-finite values (NaN or inf) by replacing them with a default value (0)\n",
    "        data['clickedOrNot'] = data['clickedOrNot'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "        \n",
    "        # Convert the column to integers\n",
    "        data['clickedOrNot'] = data['clickedOrNot'].astype(int)\n",
    "        \n",
    "        data = pd.DataFrame(data)\n",
    "        print(data.head())\n",
    "        print(\"Data processed successfully.\")\n",
    "      \n",
    "        return data\n",
    "\n",
    "# Example usage:\n",
    "csv_file_path = \"advertisement_final.csv\"\n",
    "\n",
    "\n",
    "# Create an instance of DataProcessing to preprocess the data\n",
    "data_processor = DataProcessing(csv_file_path)\n",
    "processed_data = data_processor.process_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Build_Pipeline.data_preprocessing import DataProcessing\n",
    "\n",
    "class DataSplitter:\n",
    "    @staticmethod\n",
    "    def split_data(csv_file_path, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "            csv_file_path (str): Path to the CSV file containing the dataset.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "            random_state (int): Controls the shuffling applied to the data before splitting.\n",
    "\n",
    "        Returns:\n",
    "            train_set (DataFrame): The training set.\n",
    "            test_set (DataFrame): The testing set.\n",
    "        \"\"\"\n",
    "        # Process the CSV file to get a DataFrame\n",
    "        data_processor = DataProcessing(csv_file_path)\n",
    "        data = data_processor.process_data()\n",
    "        \n",
    "        print(\"Before calling split_data method\")\n",
    " \n",
    "        # Select features (X) and target variable (y)\n",
    "        X = data[['visitorEmail', 'ad_id']]  # Features\n",
    "        y = data['clickedOrNot']  # Target variable\n",
    "\n",
    "        # Split dataset into training set and test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "        print(\"After calling split_data method\")\n",
    "\n",
    "        # Concatenate features and target variable to create train_set and test_set\n",
    "        train_set = pd.concat([X_train, y_train], axis=1)\n",
    "        test_set = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        print(\"Data split successful.\")  # Print statement to confirm data split\n",
    "        \n",
    "        return train_set, test_set\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             visitorEmail                     ad_id  clickedOrNot\n",
      "0      promod@yopmail.com  64d4e9ff3709c058de9809ec             1\n",
      "1  kalipopup1@yopmail.com  64d4e66e3709c058de9809cb             0\n",
      "2  kalipopup1@yopmail.com  64d9af723709c058de980f8d             0\n",
      "3  kalipopup1@yopmail.com  64d9af9d3709c058de980f94             0\n",
      "4  kalipopup1@yopmail.com  64d9af4d3709c058de980f86             0\n",
      "Data processed successfully.\n",
      "Data split successful.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Build_Pipeline.data_preprocessing import DataProcessing\n",
    "\n",
    "# Path to the CSV file containing the dataset\n",
    "csv_file_path = \"advertisement_final.csv\"\n",
    "\n",
    "# Process the CSV file to get a DataFrame\n",
    "data_processor = DataProcessing(csv_file_path)\n",
    "data = data_processor.process_data()\n",
    "\n",
    "# Select features (X) and target variable (y)\n",
    "X = data[['visitorEmail', 'ad_id']]  # Features\n",
    "y = data['clickedOrNot']  # Target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "test_size = 0.2\n",
    "random_state = 42  # Set to None if you don't want to use a random seed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Concatenate features and target variable to create train_set and test_set\n",
    "train_set = pd.concat([X_train, y_train], axis=1)\n",
    "test_set = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "print(\"Data split successful.\")  # Print statement to confirm data split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n",
      "Precision at 10: 0.13333334028720856\n",
      "Recall at 10: 0.4107188743497779\n"
     ]
    }
   ],
   "source": [
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "from Build_Pipeline.data_splitting import train_set, test_set\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "num_factors = 3\n",
    "num_epochs = 150\n",
    "learning_rate = 0.05\n",
    "loss = 'warp'\n",
    "item_alpha = 0.0001\n",
    "user_alpha = 0.0001\n",
    "\n",
    "# Initialize the LightFM model\n",
    "model = LightFM(no_components=num_factors, loss=loss, learning_rate=learning_rate, item_alpha=item_alpha, user_alpha=user_alpha)\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset()\n",
    "\n",
    "# Fit the dataset on the train_set DataFrame to create the user and item indices\n",
    "dataset.fit((user for user in train_set['visitorEmail']),\n",
    "            (item for item in train_set['ad_id']))\n",
    "\n",
    "# Build the interaction matrix\n",
    "(interactions, weights) = dataset.build_interactions(((row['visitorEmail'], row['ad_id']) for index, row in train_set.iterrows()))\n",
    "\n",
    "# Train the model\n",
    "model.fit(interactions, epochs=num_epochs)\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "k = 10\n",
    "# Filter test set to include only user IDs and item IDs present in the training set\n",
    "filtered_test_set = test_set[(test_set['visitorEmail'].isin(train_set['visitorEmail'])) & (test_set['ad_id'].isin(train_set['ad_id']))]\n",
    "\n",
    "# Convert the filtered test set to interactions\n",
    "test_interactions, _ = dataset.build_interactions(((row['visitorEmail'], row['ad_id']) for index, row in filtered_test_set.iterrows()))\n",
    "\n",
    "# Calculate precision at k\n",
    "precision = precision_at_k(model, test_interactions, k=k).mean()\n",
    "\n",
    "# Calculate recall at k\n",
    "recall = recall_at_k(model, test_interactions, k=k).mean()\n",
    "\n",
    "print(f\"Precision at {k}: {precision}\")\n",
    "print(f\"Recall at {k}: {recall}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
